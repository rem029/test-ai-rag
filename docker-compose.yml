version: "3.9"

services:
  pgvector:
    image: pgvector/pgvector:pg16
    container_name: pgvector
    restart: unless-stopped
    ports:
      - "${PG_PORT}:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - pgvector-data:/var/lib/postgresql/data

  ollama:
    image: ollama/ollama:rocm
    container_name: ollama
    restart: unless-stopped
    ports:
      - "${OLLAMA_PORT}:11434"
    environment:
      OLLAMA_HOST: 0.0.0.0
    devices:
      - "/dev/kfd:/dev/kfd"
      - "/dev/dri:/dev/dri"
    volumes:
      - ollama:/root/.ollama
    command: serve

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    ports:
      - "${WEBUI_PORT}:8080"
    environment:
      OLLAMA_BASE_URL: http://ollama:11434
    volumes:
      - open-webui:/app/backend/data
    depends_on:
      - ollama

  llama-full:
    image: ghcr.io/ggml-org/llama.cpp:full-vulkan
    container_name: llama-full
    stdin_open: true
    tty: true
    devices:
      - "/dev/dri:/dev/dri"
    volumes:
      - ./models:/models
    command:
      ["--all-in-one", "/models", "7B"]
      # This will attempt to download and prepare LLaMA 7B
      # on the first container run. Requires Meta license acceptance.

  llama-qwen2-5vl-3b:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: llama-qwen2-5vl-3b
    restart: unless-stopped
    ports:
      - "9001:8000"
    devices:
      - "/dev/dri:/dev/dri"
    volumes:
      - ./models:/models
    command:
      [
        "-m",
        "/models/qwen2.5VL-3B-Instruct-Q4_K_M.gguf",
        "--mmproj",
        "/models/mmproj-F16.gguf",
        "--port",
        "8000",
        "--host",
        "0.0.0.0",
      ]

  llama-nomic-embedv1-5:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: llama-nomic-embedv1-5
    restart: unless-stopped
    ports:
      - "9002:8000"
    devices:
      - "/dev/dri:/dev/dri"
    volumes:
      - ./models:/models
    command:
      [
        "-m",
        "/models/nomic-embed-text-v1.5.Q4_K_M.gguf",
        "--port",
        "8000",
        "--host",
        "0.0.0.0",
      ]

  llama-moondream2:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: llama-moondream2
    restart: unless-stopped
    ports:
      - "9003:8000"
    devices:
      - "/dev/dri:/dev/dri"
    volumes:
      - ./models:/models
    command:
      [
        "-m",
        "/models/moondream2-text-model-f16.gguf",
        "--mmproj",
        "/models/moondream2-mmproj-f16.gguf",
        "--port",
        "8000",
        "--host",
        "0.0.0.0",
      ]

  llama-gemma3-4b:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: llama-gemma34b
    restart: unless-stopped
    ports:
      - "9001:8000"
    devices:
      - "/dev/dri:/dev/dri"
    volumes:
      - ./models:/models
    command:
      [
        "-m",
        "/models/gemma-3-4b-it-Q4_K_M.gguf",
        "--mmproj",
        "/models/gemma-3-4b-it-Q4_K_M-mmproj.gguf",
        "--port",
        "8000",
        "--host",
        "0.0.0.0",
      ]

volumes:
  pgvector-data:
  ollama:
  open-webui:
