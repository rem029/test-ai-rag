version: "3.9"

services:
  pgvector:
    image: pgvector/pgvector:pg16
    container_name: pgvector
    restart: unless-stopped
    ports:
      - "${PG_PORT}:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - pgvector-data:/var/lib/postgresql/data

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    ports:
      - "${WEBUI_PORT}:8080"
    environment:
      OLLAMA_BASE_URL: http://ollama:11434
    volumes:
      - open-webui:/app/backend/data

  # CUDA Services (for NVIDIA GPUs)
  llama-full-cuda:
    image: ghcr.io/ggml-org/llama.cpp:full-cuda
    container_name: llama-full
    stdin_open: true
    tty: true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_DEVICE_COUNT:-1}
              capabilities: [gpu]
    volumes:
      - ./models:/models
    command:
      ["--all-in-one", "/models", "7B"]
    profiles:
      - cuda

  llama-embed-cuda:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: llama-embed
    restart: unless-stopped
    ports:
      - "${MODEL_EMBED_PORT}:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_DEVICE_COUNT:-1}
              capabilities: [gpu]
    volumes:
      - ./models:/models
    command:
      [
        "-m",
        "/models/nomic-embed-text-v1.5.Q4_K_M.gguf",
        "--port",
        "8000",
        "--host",
        "0.0.0.0",
        "--embeddings",
      ]
    profiles:
      - cuda

  llama-vision-cuda:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: llama-vision
    restart: unless-stopped
    ports:
      - "${MODEL_MM_PORT}:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_DEVICE_COUNT:-1}
              capabilities: [gpu]
    volumes:
      - ./models:/models
    command:
      [
        "-m",
        "/models/gemma-3-4b-it-Q4_K_M.gguf",
        "--mmproj",
        "/models/gemma-3-4b-it-Q4_K_M-mmproj.gguf",
        "--port",
        "8000",
        "--host",
        "0.0.0.0",
      ]
    profiles:
      - cuda

  # Vulkan Services (for AMD GPUs, Intel Arc, or fallback)
  llama-full-vulkan:
    image: ghcr.io/ggml-org/llama.cpp:full-vulkan
    container_name: llama-full
    stdin_open: true
    tty: true
    devices:
      - "/dev/dri:/dev/dri"
    volumes:
      - ./models:/models
    command:
      ["--all-in-one", "/models", "7B"]
    profiles:
      - vulkan

  llama-embed-vulkan:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: llama-embed
    restart: unless-stopped
    ports:
      - "${MODEL_EMBED_PORT}:8000"
    devices:
      - "/dev/dri:/dev/dri"
    volumes:
      - ./models:/models
    command:
      [
        "-m",
        "/models/nomic-embed-text-v1.5.Q4_K_M.gguf",
        "--port",
        "8000",
        "--host",
        "0.0.0.0",
        "--embeddings",
      ]
    profiles:
      - vulkan

  llama-vision-vulkan:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: llama-vision
    restart: unless-stopped
    ports:
      - "${MODEL_MM_PORT}:8000"
    devices:
      - "/dev/dri:/dev/dri"
    volumes:
      - ./models:/models
    command:
      [
        "-m",
        "/models/gemma-3-4b-it-Q4_K_M.gguf",
        "--mmproj",
        "/models/gemma-3-4b-it-Q4_K_M-mmproj.gguf",
        "--port",
        "8000",
        "--host",
        "0.0.0.0",
      ]
    profiles:
      - vulkan

volumes:
  pgvector-data:
  open-webui:
