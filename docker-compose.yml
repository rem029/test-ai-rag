version: "3.9"

services:
  pgvector:
    image: pgvector/pgvector:pg16
    container_name: pgvector
    restart: unless-stopped
    ports:
      - "${PG_PORT}:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - pgvector-data:/var/lib/postgresql/data

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    ports:
      - "${WEBUI_PORT}:8080"
    environment:
      OLLAMA_BASE_URL: http://ollama:11434
    volumes:
      - open-webui:/app/backend/data

    # --- SearxNG meta-search + Redis (for RAG discovery) ---
  searxng-redis:
    image: redis:7-alpine
    container_name: searxng-redis
    restart: unless-stopped

  searxng:
    image: docker.io/searxng/searxng:latest
    container_name: searxng
    depends_on:
      - searxng-redis
    restart: unless-stopped
    ports:
      - "${SEARX_PORT:-8888}:8080"
    environment:
      SEARXNG_BASE_URL: "http://localhost:${SEARX_PORT:-8888}/"
      SEARXNG_BIND_ADDRESS: "0.0.0.0"
      SEARXNG_PORT: "8080"
      TZ: "UTC"
    volumes:
      - searxng-config:/etc/searxng
      - searxng-data:/var/cache/searxng

  # CUDA Services (for NVIDIA GPUs)
  llama-full-cuda:
    image: ghcr.io/ggml-org/llama.cpp:full-cuda
    container_name: llama-full
    stdin_open: true
    tty: true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_DEVICE_COUNT:-1}
              capabilities: [gpu]
    volumes:
      - ./models:/models
    command: ["--all-in-one", "/models", "7B"]
    profiles:
      - cuda

  llama-embed-cuda:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: llama-embed
    restart: unless-stopped
    ports:
      - "${MODEL_EMBED_PORT}:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_DEVICE_COUNT:-1}
              capabilities: [gpu]
    volumes:
      - ./models:/models
    command:
      [
        "-m",
        "/models/nomic-embed-text-v1.5.Q4_K_M.gguf",
        "--port",
        "8000",
        "--host",
        "0.0.0.0",
        "--embeddings",
        "--batch-size", "256",     # safe start for 6GB
        "--ubatch-size", "32",     # try 16; if stable, raise to 32
        "--parallel", "1"          # avoid concurrent spikes
      ]
    profiles:
      - cuda

  llama-vision-cuda:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: llama-vision
    restart: unless-stopped
    ports:
      - "${MODEL_MM_PORT}:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_DEVICE_COUNT:-1}
              capabilities: [gpu]
    volumes:
      - ./models:/models
    command:
      [
        "-m",
        "/models/${MODEL_VISION_MAIN_FILENAME}",
        "--mmproj",
        "/models/${MODEL_VISION_FILENAME}",
        "--port",
        "8000",
        "--host",
        "0.0.0.0",
      ]
    profiles:
      - cuda

  # Vulkan Services (for AMD GPUs, Intel Arc, or fallback)
  llama-full-vulkan:
    image: ghcr.io/ggml-org/llama.cpp:full-vulkan
    container_name: llama-full
    stdin_open: true
    tty: true
    devices:
      - "/dev/dri:/dev/dri"
    volumes:
      - ./models:/models
    command: ["--all-in-one", "/models", "7B"]
    profiles:
      - vulkan

  llama-embed-vulkan:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: llama-embed
    restart: unless-stopped
    ports:
      - "${MODEL_EMBED_PORT}:8000"
    devices:
      - "/dev/dri:/dev/dri"
    volumes:
      - ./models:/models
    command:
      [
        "-m",
        "/models/${MODEL_EMBED_FILENAME}",
        "--port",
        "8000",
        "--host",
        "0.0.0.0",
        "--embeddings",
        "--batch-size", "128",     # safe start for 6GB
        "--ubatch-size", "16",     # try 16; if stable, raise to 32
        "--parallel", "1"          # avoid concurrent spikes
      ]
    profiles:
      - vulkan

  llama-vision-vulkan:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: llama-vision
    restart: unless-stopped
    ports:
      - "${MODEL_MM_PORT}:8000"
    devices:
      - "/dev/dri:/dev/dri"
    volumes:
      - ./models:/models
    command:
      [
        "-m",
        "/models/${MODEL_VISION_MAIN_FILENAME}",
        "--mmproj",
        "/models/${MODEL_VISION_FILENAME}",
        "--port",
        "8000",
        "--host",
        "0.0.0.0",
      ]
    profiles:
      - vulkan

  llama-coder-cuda:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: llama-coder
    restart: unless-stopped
    ports:
      - "${MODEL_CODER_PORT:-8001}:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_DEVICE_COUNT:-1}
              capabilities: [gpu]
    volumes:
      - ./models:/models
    command:
      [
        "-m", "/models/${MODEL_CODER_FILENAME}",
        "--port", "8000",
        "--host", "0.0.0.0",
        "-ngl", "99",          # <--- CRITICAL: Offloads layers to GPU
        "-fa",                 # Flash Attention (saves VRAM, boosts speed)
        "-c", "8192",          # Limits context to keep VRAM usage stable
        "--ubatch-size", "256" # Smaller batch for smoother laptop performance
      ]
    profiles:
      - cuda
      - coder

  # llama-coder-vulkan:
  #   image: ghcr.io/ggml-org/llama.cpp:server-vulkan
  #   container_name: llama-coder
  #   restart: unless-stopped
  #   ports:
  #     - "${MODEL_CODER_PORT:-8001}:8000"
  #   devices:
  #     - "/dev/dri:/dev/dri"
  #   volumes:
  #     - ./models:/models
  #   command:
  #     [
  #       "-m",
  #       "/models/${MODEL_CODER_FILENAME}",
  #       "--port",
  #       "8000",
  #       "--host",
  #       "0.0.0.0",
  #     ]
  #   profiles:
  #     - vulkan
  #     - coder

volumes:
  pgvector-data:
  open-webui:
  searxng-config:
  searxng-data:
