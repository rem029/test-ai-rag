version: "3.9"

services:
  pgvector:
    image: pgvector/pgvector:pg16
    container_name: pgvector
    restart: unless-stopped
    ports:
      - "${PG_PORT}:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - pgvector-data:/var/lib/postgresql/data

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    ports:
      - "${WEBUI_PORT}:8080"
    environment:
      OLLAMA_BASE_URL: http://ollama:11434
    volumes:
      - open-webui:/app/backend/data

    # --- SearxNG meta-search + Redis (for RAG discovery) ---
  searxng-redis:
    image: redis:7-alpine
    container_name: searxng-redis
    restart: unless-stopped

  searxng:
    image: docker.io/searxng/searxng:latest
    container_name: searxng
    depends_on:
      - searxng-redis
    restart: unless-stopped
    ports:
      - "${SEARX_PORT:-8888}:8080"
    environment:
      SEARXNG_BASE_URL: "http://localhost:${SEARX_PORT:-8888}/"
      SEARXNG_BIND_ADDRESS: "0.0.0.0"
      SEARXNG_PORT: "8080"
      TZ: "UTC"
    volumes:
      - searxng-config:/etc/searxng
      - searxng-data:/var/cache/searxng

  # CUDA Services (for NVIDIA GPUs)
  llama-full-cuda:
    image: ghcr.io/ggml-org/llama.cpp:full-cuda
    container_name: llama-full
    stdin_open: true
    tty: true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_DEVICE_COUNT:-1}
              capabilities: [gpu]
    volumes:
      - ./models:/models
    command: ["--all-in-one", "/models", "7B"]
    profiles:
      - cuda

  llama-embed-cuda:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: llama-embed
    restart: unless-stopped
    ports:
      - "${MODEL_EMBED_PORT}:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_DEVICE_COUNT:-1}
              capabilities: [gpu]
    volumes:
      - ./models:/models
    command:
      [
        "-m",
        "/models/nomic-embed-text-v1.5.Q4_K_M.gguf",
        "--port",
        "8000",
        "--host",
        "0.0.0.0",
        "--embeddings",
      ]
    profiles:
      - cuda

  llama-vision-cuda:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: llama-vision
    restart: unless-stopped
    ports:
      - "${MODEL_MM_PORT}:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_DEVICE_COUNT:-1}
              capabilities: [gpu]
    volumes:
      - ./models:/models
    command:
      [
        "-m",
        "/models/${MODEL_VISION_MAIN_FILENAME}",
        "--mmproj",
        "/models/${MODEL_VISION_FILENAME}",
        "--port",
        "8000",
        "--host",
        "0.0.0.0",
      ]
    profiles:
      - cuda

  # Vulkan Services (for AMD GPUs, Intel Arc, or fallback)
  llama-full-vulkan:
    image: ghcr.io/ggml-org/llama.cpp:full-vulkan
    container_name: llama-full
    stdin_open: true
    tty: true
    devices:
      - "/dev/dri:/dev/dri"
    volumes:
      - ./models:/models
    command: ["--all-in-one", "/models", "7B"]
    profiles:
      - vulkan

  llama-embed-vulkan:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: llama-embed
    restart: unless-stopped
    ports:
      - "${MODEL_EMBED_PORT}:8000"
    devices:
      - "/dev/dri:/dev/dri"
    volumes:
      - ./models:/models
    command:
      [
        "-m",
        "/models/${MODEL_EMBED_FILENAME}",
        "--port",
        "8000",
        "--host",
        "0.0.0.0",
        "--embeddings",
      ]
    profiles:
      - vulkan

  llama-vision-vulkan:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: llama-vision
    restart: unless-stopped
    ports:
      - "${MODEL_MM_PORT}:8000"
    devices:
      - "/dev/dri:/dev/dri"
    volumes:
      - ./models:/models
    command:
      [
        "-m",
        "/models/${MODEL_VISION_MAIN_FILENAME}",
        "--mmproj",
        "/models/${MODEL_VISION_FILENAME}",
        "--port",
        "8000",
        "--host",
        "0.0.0.0",
      ]
    profiles:
      - vulkan

volumes:
  pgvector-data:
  open-webui:
  searxng-config:
  searxng-data:
