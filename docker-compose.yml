version: "3.9"

services:
  pgvector:
    image: pgvector/pgvector:pg16
    container_name: pgvector
    restart: unless-stopped
    ports:
      - "${PG_PORT}:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - pgvector-data:/var/lib/postgresql/data

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    ports:
      - "${WEBUI_PORT}:8080"
    environment:
      OLLAMA_BASE_URL: http://ollama:11434
    volumes:
      - open-webui:/app/backend/data

  llama-full:
    image: ghcr.io/ggml-org/llama.cpp:full-vulkan
    container_name: llama-full
    stdin_open: true
    tty: true
    devices:
      - "/dev/dri:/dev/dri"
    volumes:
      - ./models:/models
    command:
      ["--all-in-one", "/models", "7B"]
      # This will attempt to download and prepare LLaMA 7B
      # on the first container run. Requires Meta license acceptance.

  llama-embed:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: llama-embed
    restart: unless-stopped
    ports:
      - "${MODEL_EMBED_PORT}:8000"
    devices:
      - "/dev/dri:/dev/dri"
    volumes:
      - ./models:/models
    command:
      [
        "-m",
        "/models/nomic-embed-text-v1.5.Q4_K_M.gguf",
        "--port",
        "8000",
        "--host",
        "0.0.0.0",
        "--embeddings",
      ]

  llama-vision:
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: llama-vision
    restart: unless-stopped
    ports:
      - "${MODEL_MM_PORT}:8000"
    devices:
      - "/dev/dri:/dev/dri"
    volumes:
      - ./models:/models
    command:
      [
        "-m",
        "/models/gemma-3-4b-it-Q4_K_M.gguf",
        "--mmproj",
        "/models/gemma-3-4b-it-Q4_K_M-mmproj.gguf",
        "--port",
        "8000",
        "--host",
        "0.0.0.0",
      ]

volumes:
  pgvector-data:
  open-webui:
